[project]
name = "uv-install-vllm-omni"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.10,<3.14"
dependencies = [
    "torch==2.9.0",
    "torchaudio==2.9.0",
    "torchvision==0.24.0",
    "flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.0/flash_attn-2.8.3+cu128torch2.9-cp310-cp310-linux_x86_64.whl ; python_version == '3.10'",
    "flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.0/flash_attn-2.8.3+cu128torch2.9-cp311-cp311-linux_x86_64.whl ; python_version == '3.11'",
    "flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.0/flash_attn-2.8.3+cu128torch2.9-cp312-cp312-linux_x86_64.whl ; python_version == '3.12'",
    "flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.8.3+cu128torch2.9-cp313-cp313-linux_x86_64.whl ; python_version == '3.13'",
    "flashinfer-python==0.5.3",
    "flashinfer-cubin==0.5.3",
    "flashinfer-jit-cache==0.5.3",
    "qwen-omni-utils==0.0.8",
    "vllm==0.12.0",
    "vllm-omni==0.12.0rc1",
]

[tool.uv]
environments = [
    "sys_platform == 'linux'",
]

[tool.uv.sources]
torch = { index = "pytorch-cu128" }
torchaudio = { index = "pytorch-cu128" }
torchvision = { index = "pytorch-cu128" }
flashinfer-jit-cache = { index = "flashinfer-cu128" }

[[tool.uv.index]]
name = "pytorch-cu128"
url = "https://download.pytorch.org/whl/cu128"
explicit = true

[[tool.uv.index]]
name = "flashinfer-cu128"
url = "https://flashinfer.ai/whl/cu128"
explicit = true
